{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Loop through each tissue file and process them\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tissue_file \u001b[38;5;129;01min\u001b[39;00m tissue_files:\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mprocess_tissue_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtissue_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mprocess_tissue_autoencoder\u001b[0;34m(tissue_file, metadata)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Load normalized read counts\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tissue_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 46\u001b[0m     normalized_counts \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m sample_ids \u001b[38;5;241m=\u001b[39m normalized_counts\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     49\u001b[0m attr_filtered \u001b[38;5;241m=\u001b[39m metadata[metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(sample_ids)]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.pandas2ri as pandas2ri\n",
    "pandas2ri.activate()\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./data/processed/expression/adjusted_autoencoder\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define data paths\n",
    "data_path = \"./data/processed/expression/readcounts_tmm_all/\"\n",
    "metadata_path = \"./data/processed/attphe.pkl\"\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# List all tissue files\n",
    "tissue_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "# Define Standard Autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),  # Latent space\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='sigmoid')  # Reconstruction layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Function to process each tissue file\n",
    "def process_tissue_autoencoder(tissue_file, metadata):\n",
    "    tissue_name = os.path.basename(tissue_file).replace(\".pkl\", \"\")\n",
    "    \n",
    "    # Load normalized read counts\n",
    "    with open(tissue_file, 'rb') as f:\n",
    "        normalized_counts = pickle.load(f)\n",
    "    \n",
    "    sample_ids = normalized_counts.columns\n",
    "    attr_filtered = metadata[metadata['sample_id'].isin(sample_ids)]\n",
    "    \n",
    "    if normalized_counts.shape[1] != attr_filtered.shape[0]:\n",
    "        raise ValueError(f\"Number of samples in {tissue_name} does not match metadata.\")\n",
    "    \n",
    "    # Resave normalized counts as DataFrame (using pandas HDF5 as previously)\n",
    "    resaved_tissue_file = os.path.join(data_path, f\"{tissue_name}_resaved.h5\")\n",
    "    normalized_counts.to_hdf(resaved_tissue_file, key='normalized_counts')\n",
    "    print(f\"Resaved normalized counts for {tissue_name} to {resaved_tissue_file}\")\n",
    "    \n",
    "    input_dim = normalized_counts.shape[1]\n",
    "    autoencoder = build_autoencoder(input_dim)\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(normalized_counts.to_numpy(), normalized_counts.to_numpy(), epochs=50, batch_size=32)\n",
    "    \n",
    "    encoder = keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)  # Extract latent space\n",
    "    latent_features = encoder.predict(normalized_counts.to_numpy())\n",
    "    \n",
    "    # Convert DataFrame and latent features to Python-native lists (not Rpy2)\n",
    "    normalized_counts_list = normalized_counts.values.tolist()  # Convert to list\n",
    "    latent_features_list = latent_features.tolist()  # Convert to list\n",
    "\n",
    "    # Load limma package in R\n",
    "    ro.r('library(limma)')\n",
    "    \n",
    "    # Adjust expression data using removeBatchEffect\n",
    "    adjusted_expression_data = ro.r['removeBatchEffect'](normalized_counts_list, covariates=latent_features_list)\n",
    "\n",
    "    # Convert adjusted data back to pandas DataFrame\n",
    "    adjusted_expression_df = pd.DataFrame(np.array(adjusted_expression_data), \n",
    "                                          index=normalized_counts.index, \n",
    "                                          columns=normalized_counts.columns)\n",
    "    \n",
    "    # Save adjusted expression data to pickle file\n",
    "    result_file = os.path.join(output_dir, f\"{tissue_name}.pkl\")\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(adjusted_expression_df, f)\n",
    "    \n",
    "    print(f\"Processed tissue: {tissue_name}\")\n",
    "    print(f\"Dimensions of adjusted data: {adjusted_expression_df.shape}\")\n",
    "\n",
    "# Loop through each tissue file and process them\n",
    "for tissue_file in tissue_files:\n",
    "    process_tissue_autoencoder(tissue_file, metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
