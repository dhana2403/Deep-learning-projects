{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.pandas2ri as pandas2ri\n",
    "pandas2ri.activate()\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./data/processed/expression/adjusted_autoencoder\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define data paths\n",
    "data_path = \"./data/processed/expression/readcounts_tmm_all/\"\n",
    "metadata_path = \"./data/processed/attphe.pkl\"\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# List all tissue files\n",
    "tissue_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "# Define Standard Autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),  # Latent space\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='sigmoid')  # Reconstruction layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Function to process each tissue file\n",
    "def process_tissue_autoencoder(tissue_file, metadata):\n",
    "    tissue_name = os.path.basename(tissue_file).replace(\".pkl\", \"\")\n",
    "    \n",
    "    # Load normalized read counts\n",
    "    with open(tissue_file, 'rb') as f:\n",
    "        normalized_counts = pickle.load(f)\n",
    "    \n",
    "    sample_ids = normalized_counts.columns\n",
    "    attr_filtered = metadata[metadata['sample_id'].isin(sample_ids)]\n",
    "    \n",
    "    if normalized_counts.shape[1] != attr_filtered.shape[0]:\n",
    "        raise ValueError(f\"Number of samples in {tissue_name} does not match metadata.\")\n",
    "    \n",
    "    # Resave normalized counts as DataFrame (using pandas HDF5 as previously)\n",
    "    resaved_tissue_file = os.path.join(data_path, f\"{tissue_name}_resaved.h5\")\n",
    "    normalized_counts.to_hdf(resaved_tissue_file, key='normalized_counts')\n",
    "    print(f\"Resaved normalized counts for {tissue_name} to {resaved_tissue_file}\")\n",
    "    \n",
    "    input_dim = normalized_counts.shape[1]\n",
    "    autoencoder = build_autoencoder(input_dim)\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(normalized_counts.to_numpy(), normalized_counts.to_numpy(), epochs=50, batch_size=32)\n",
    "    \n",
    "    encoder = keras.Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)  # Extract latent space\n",
    "    latent_features = encoder.predict(normalized_counts.to_numpy())\n",
    "    \n",
    "    # Convert DataFrame and latent features to Python-native lists (not Rpy2)\n",
    "    normalized_counts_list = normalized_counts.values.tolist()  # Convert to list\n",
    "    latent_features_list = latent_features.tolist()  # Convert to list\n",
    "\n",
    "    # Load limma package in R\n",
    "    ro.r('library(limma)')\n",
    "    \n",
    "    # Adjust expression data using removeBatchEffect\n",
    "    adjusted_expression_data = ro.r['removeBatchEffect'](normalized_counts_list, covariates=latent_features_list)\n",
    "\n",
    "    # Convert adjusted data back to pandas DataFrame\n",
    "    adjusted_expression_df = pd.DataFrame(np.array(adjusted_expression_data), \n",
    "                                          index=normalized_counts.index, \n",
    "                                          columns=normalized_counts.columns)\n",
    "    \n",
    "    # Save adjusted expression data to pickle file\n",
    "    result_file = os.path.join(output_dir, f\"{tissue_name}.pkl\")\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(adjusted_expression_df, f)\n",
    "    \n",
    "    print(f\"Processed tissue: {tissue_name}\")\n",
    "    print(f\"Dimensions of adjusted data: {adjusted_expression_df.shape}\")\n",
    "\n",
    "# Loop through each tissue file and process them\n",
    "for tissue_file in tissue_files:\n",
    "    process_tissue_autoencoder(tissue_file, metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
