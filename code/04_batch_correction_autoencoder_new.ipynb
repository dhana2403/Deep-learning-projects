{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: './data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create output directory\u001b[39;00m\n\u001b[1;32m     14\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed/expression/tpm/adjusted_autoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define data paths\u001b[39;00m\n\u001b[1;32m     18\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed/expression/readcounts_tmm_all/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: './data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects.pandas2ri as pandas2ri\n",
    "pandas2ri.activate()\n",
    "import pickle\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./data/processed/expression/autoencoder/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define data paths\n",
    "data_path = \"./data/processed/expression/readcounts_tmm_all/\"\n",
    "metadata_path = \"./data/processed/attphe.pkl\"\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# List all tissue files\n",
    "tissue_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "# Define Autoencoder using Functional API\n",
    "def build_autoencoder(input_dim):\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(input_layer)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    latent = layers.Dense(32, activation='relu')(x)  # Latent space\n",
    "    x = layers.Dense(64, activation='relu')(latent)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=output)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Encoder model to extract latent space\n",
    "    encoder = keras.Model(inputs=input_layer, outputs=latent)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Function to process each tissue file\n",
    "def process_tissue_autoencoder(tissue_file, metadata, min_samples=10):\n",
    "    tissue_name = os.path.basename(tissue_file).replace(\".pkl\", \"\")\n",
    "\n",
    "    # Load normalized read counts\n",
    "    with open(tissue_file, 'rb') as f:\n",
    "        normalized_counts = pickle.load(f)\n",
    "\n",
    "    sample_ids = normalized_counts.columns\n",
    "    attr_filtered = metadata[metadata['samp_id'].isin(sample_ids)]\n",
    "\n",
    "    # Skip tissue if the sample size is smaller than the minimum threshold\n",
    "    if len(sample_ids) < min_samples:\n",
    "        print(f\"Skipping tissue {tissue_name} due to small sample size ({len(sample_ids)} samples).\")\n",
    "        return\n",
    "\n",
    "    if normalized_counts.shape[1] != attr_filtered.shape[0]:\n",
    "        raise ValueError(f\"Number of samples in {tissue_name} does not match metadata.\")\n",
    "\n",
    "    # Resave normalized counts as DataFrame (using pandas HDF5 as previously)\n",
    "    resaved_tissue_file = os.path.join(data_path, f\"{tissue_name}_resaved.h5\")\n",
    "    normalized_counts.to_hdf(resaved_tissue_file, key='normalized_counts')\n",
    "    print(f\"Resaved normalized counts for {tissue_name} to {resaved_tissue_file}\")\n",
    "\n",
    "    \n",
    "    input_dim = normalized_counts.shape[1]\n",
    "    scaler = StandardScaler()\n",
    "    input_dim = scaler.fit_transform(normalized_counts.T).T  # Normalize across samples\n",
    "\n",
    "    autoencoder, encoder = build_autoencoder(input_dim)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(normalized_counts.to_numpy(), normalized_counts.to_numpy(), epochs=50, batch_size=32)\n",
    "\n",
    "    # Encode latent features\n",
    "    latent_features = encoder.predict(normalized_counts.to_numpy())\n",
    "\n",
    "    # Load limma package in R\n",
    "    ro.r('library(limma)')\n",
    "    \n",
    "    from rpy2.robjects import numpy2ri\n",
    "    numpy2ri.activate()\n",
    "    \n",
    "    # Transpose expression matrix so rows = samples, columns = genes\n",
    "    counts_T = normalized_counts.T  # shape: (samples, genes)\n",
    "    r_counts = ro.conversion.py2rpy(counts_T.to_numpy())  # \n",
    "    # Convert latent features\n",
    "    r_covariates = ro.conversion.py2rpy(np.array(latent_features))\n",
    "\n",
    "    print(\"Expression shape:\", normalized_counts.shape)\n",
    "    print(\"Latent shape:\", latent_features.shape)\n",
    "\n",
    "    # Call R function\n",
    "    adjusted_expression_data = ro.r['removeBatchEffect'](r_counts, covariates=r_covariates)\n",
    "\n",
    "    # Convert adjusted data back to pandas DataFrame and transpose it to original shape\n",
    "    adjusted_expression_df = pd.DataFrame(np.array(adjusted_expression_data),\n",
    "                                          index=counts_T.index,\n",
    "                                          columns=counts_T.columns).T  # \n",
    "    \n",
    "    # Save adjusted expression data to pickle file\n",
    "    result_file = os.path.join(output_dir, f\"{tissue_name}.pkl\")\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(adjusted_expression_df, f)\n",
    "\n",
    "    print(f\"Processed tissue: {tissue_name}\")\n",
    "    print(f\"Dimensions of adjusted data: {adjusted_expression_df.shape}\")\n",
    "\n",
    "# Loop through each tissue file and process them\n",
    "for tissue_file in tissue_files:\n",
    "    process_tissue_autoencoder(tissue_file, metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
